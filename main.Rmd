---
title: "Tidy data structure to support exploration and modeling of temporal-context data"
author:
  - familyname: Wang
    othernames: Earo
    address: Department of Econometrics and Business Statistics, \newline Monash University, VIC 3800 \newline Australia.
    email : earo.wang@monash.edu
    correspondingauthor: true
  - familyname: Cook
    othernames: Dianne
    address: Department of Econometrics and Business Statistics, \newline Monash University, VIC 3800 \newline Australia.
    email : dicook@monash.edu
  - familyname: Hyndman
    othernames: Rob J
    address: Department of Econometrics and Business Statistics, \newline Monash University, VIC 3800 \newline Australia.
    email : rob.hyndman@monash.edu
abstract: 'Mining temporal-context data for information is often inhibited by a multitude of time formats: irregular or multiple time intervals, multiple observational units or repeated measurements on multiple individuals, heterogeneous data types, nested and crossed factors indicating hierarchical sub-groups. Time series models, in particular, the software supporting time series forecasting makes strict assumptions on data that needs to be provided, typically a matrix of numeric data with an implicit time index. Going from raw data to model-ready data is painful. This work presents a cohesive and conceptual framework for organizing and manipulating temporal data, which in turn flows into visualization and forecasting routines. Tidy data principles are applied, and extended to temporal data: (1) mapping the semantics of a dataset into its physical layout, (2) an explicitly declared index variable representing time, (3) a "key" comprised of single or multiple variables to uniquely identify units over time, using a syntax-based and user-oriented approach in which it imposes nested or crossed structures on the data. This tidy data representation most naturally supports thinking of operations on the data as building blocks, forming part of a "data pipeline" in time-based context. A sound data pipeline facilitates a succinct and transparent workflow for analyzing temporal data. Applications are included to illustrate tidy temporal data structure, data pipeline structure and usage. The infrastructure of tidy temporal data has been implemented in the R package **tsibble**.'
keywords: "temporal context, time series, data structure, R"
wpnumber: no/yr
jelcodes: C10,C14,C22
blind: false
cover: true
toc: false
bibliography: [references.bib, rpkgs.bib]
biblio-style: authoryear-comp
output:
  MonashEBSTemplates::workingpaper:
    fig_caption: yes
    fig_height: 5
    fig_width: 8
    includes:
      in_header: preamble.tex
    keep_tex: no
    number_sections: yes
    citation_package: biblatex
---

```{r initial, echo = FALSE, cache = FALSE, include = FALSE}
library(knitr)
opts_chunk$set(
  warning = FALSE, message = FALSE, echo = FALSE,
  fig.path = 'figure/', cache.path = 'cache/', fig.align = 'center', 
  fig.show = 'hold', cache = TRUE, external = TRUE, dev = "svglite"
)
read_chunk('R/demo.R')
```

```{r load-pkg}
```

# Introduction {#sec:intro}

Temporal-context data consist of observational units indexed at different time points $X_{jt}$, where the $j$\textsuperscript{th} in advance unit takes measurements of $X$ on over time $t$, for $j = 1, \dots, N_{i}$ and $1 \le t \le T$. Time primarily forms the contextual basis of temporal data, but it could arrive in many possible formats. For example, data recorded at fine time resolutions (hours, minutes, and seconds) are typically associated with different time zones and daylight savings. Temporal data often carries with rich information other than the time: multiple observational units of different time lengths, multiple and heterogeneous measured variables, multiple grouping factors involving nested or crossed structures, linking to other data tables, and etc.

## Time series and longitudinal data

Time series and longitudinal data are the common terms when referring to temporal-context data. Despite of exactly the same data input, its representation diverges in these two areas due to different modelling focuses. Time series can be univariate or multivariate, but usually involves observations of relatively large in length (i.e. large $T$) possessed by each series. Time series researchers and analysts who are concerned with this large $T$ property, are mostly concerned with stochastic process, for the primary purpose of forecasting, temporal dynamics, and etc. Time series are represented as vectors or matrices in statistical software, for example MATLAB [@Matlab] and R [@R-base] (including `ts`, **zoo** [@zoo2005], and **xts** [@R-xts]). The structure the multivariate time series are in is called **wide format** where each row is assumed to hold observations at a time point and each column to contain a single time series. This implies that data columns of homogeneous types: numeric or non-numeric, but there are limited supporting methods for non-numeric cases. In addition to homogeneity, time indices must be stripped off the data and implicitly inferred as attributes or meta-information; the number of observations must be the same across all the series. Data wrangling from "raw" form to this specialist format can be frustrating and difficult, in order to perform a variety of downstream tasks such as analytics.

On the other hand, those who are engaged in longitudinal design, are primarily interested in explaining variations among individuals in change process and making inference about the underlying population. Longitudinal data or panel data typically refers to fewer measurements (small $T$) over a large number of individuals (large $N$). It often occurs to longitudinal design that some individuals are not observed at certain points in time, resulting in an unbalanced panel. This data issue leads representing longitudinal data much more commonly in **long format** where each row corresponds to an observational unit per time and each column contains measurements across the individuals for all points in time. Evidently, this data organisation saves storage space for many sparse cells, compared to structuring it in that wide format. Another appealing feature for a long format is data structured in a concise and semantic manner with reference to observations and variables. This supports perceiving time index as an explicit variable instead of an implicit attribute, and subsequently opens the door to manipulating time such as extracting time components. In general, fewer restrictions are made if representing temporal-context data in long format. 

Longitudinal data representation has been implemented in Stata's time series module [@Stata] and R package **plm** [@plm2008]. The underpinning data structure is a two-dimensional column-homogeneous array, as other tabular data. Specifying a longitudinal data set from a generic array needs explicitly declaring time and individuals (or panel variable in Stata's `tsset` command). Therefore, each row of the data can be identified by a specific individual and time point. Individuals, however, can be only declared through a single variable, not multiple.

## Time series of nested and crossed factors

A collection of time series can often be intrinsically arranged along multivariate data of nested and crossed factors, which is also known as hierarchical and grouped time series [@fpp]. For example, from an operational point of view, a manufacturer can add up every store’s sales by region, by state, and by country along a geographical tree; alternatively, one can group the sales for each product together based on common attributes such as store, brand, price range and so forth, resulting in a non-hierarchical structure. The former gives an example of hierarchical time series, and the latter shows grouped time series. And even when collections of time series are absent from hierarchical structure, it is possible to arrange them empirically into a hierarchy such as regression tree [@sankaran_interactive_2017].

The R package **hts** [@R-hts] is the implementation of structured collections of time series, and provides tools for visualizing, analyzing, and forecasting such data. But it is a pain to configure node list or grouping matrix as the increase in hierarchic depth and grouping factors. This is resulted from the fact that the structure specification is separated from data itself and represented numerically, which is rather counterintuitive from the user's perspective.

## Tidy data and the grammar of data manipulation

@wickham2014tidy coined the term "tidy data", which is a rephrasing of the second and third normal forms in relational databases. and formalized the processing from messy data to "tidy data". These principles attempt to standardize the mapping from the semantics of a dataset to its structure and facilitate data analysis in a coherent way. Based on the systematic structuring principles, the grammar of data manipulation, as implemented in the R package **dplyr** [@R-dplyr], abstracts an coherent and consistent set of methods to handle a wide range of data transformation tasks. @r4ds argues that 80% of data analysis tasks can be solved with tidy tools while the remaining 20% requires other tools. [R]

This paper proposes a unified data representation of temporal-context data, blending time series of nested and crossed factors into a two-dimensional column-homogeneous array in a long format. By leveraging the "tidy data" principles, observations and variables position and bridge their meanings in both physical and internal structures. Data manipulation involves in transforming either observations or variables, or both, which can be described and achieved with a collection of shorthand operators. A chain of data transformations lend itself to a data pipeline. 

The rest of the paper is structured as follows.

# Data semantics {#sec:semantics}

The choice of representation of temporal-context data is made from a data-centric perspective, which is taken in the light of the operations that are to be performed on the data. This data abstraction reflects most common problems of incoming data and transformation in reality. Firstly, a data set must be structured in a "tidy" rectangular layout each row has the same schema and each field has a single value. Secondly, declaring the data set to contain temporal observations is determined by an "index" that represents time and a "key" that uniquely identifies each unit that measurements take place on over time. The "key" serves a similar purpose as the panel variable in the Stata's **tsset** command to define the units or subjects, but it is expanded to include multiple variables rather than a single one. A syntax is introduced to express a key consisting of nested and crossed factors. The composition of index and key uniquely defines each observation in a data table, which is equivalent to a primary key [@codd_relational_1970] in a relational database. (The matrix representation also implies that every observation is determined by column variable and time-indexed row.)

Table \@ref(tab:tb-sub) shows a "tidy" temporal data example using a subset of tuberculosis cases estimated by @tb-data. It contains 12 observations and 5 variables arranged in a "long" tabular form. Each observation uniquely records the number of people, who are diagnosed tuberculosis for each gender at three selected countries in the years of 2011 and 2012, where column `year` is declared as the index variable, columns `country`, `continent` and `gender` as key. To be more specific, the key is passed via `id(country | continent, gender)`, which produces a geographic hierarchy of country nested within continent, crossed with a demographic factor. Detailed discussions about index and key specifications will be given in Section \@ref(sec:index) and \@ref(sec:key). Although column `count` is the only measure in this case, it is sufficiently flexible to hold other measured variables, for example, adding the corresponding population size (if known) in order to calibrate the count later.

Given the nature of temporal ordering, a temporal data set must be sorted by time index as a result. If a key is explicitly declared, the key will be sorted first depending on the ordering of factor levels and the ordering of variables, and followed by arranging time in past-to-future order. The tuberculosis example shows the key is sorted in alphabetic order, followed by ascending years by default, as in Table \@ref(tab:tb-sub). Since time-based operations, for example leading, lagging, or differencing series, require a vector in its time order, the arranged data format assures its validity for these operations without returning unexpected results. There are other possible arrangements in the key, but the baseline is the index (i.e. `year`) ascends within each key element. If any operations on the tidy temporal data results in a deviation from this baseline, it will issue a warning to the users.

This high-level data abstraction is semantically structured, which shed lights on time index and what we call "key".

```{r tb-sub}
```

(ref:tb-sub) A small subset of estimates of tuberculosis burden generated by World Health Organisation in 2011 and 2012, with 12 observations and 5 variables. The index refers to column `year`, the key to multiple columns: `contry` nested under `continent` crossed with `gender`, and the measured variable to column `count`.

## Time index and interval {#sec:index}

<!-- note to self: There's a trade-off between data column and attributes. Data column is easily accessible, which gives greater flexibility; however, once the value in the data column is modified, no way to validate for any static approach. Generally, not recommended to modify attributes. -->

Time forms an integral component and a contextual basis of temporal data. A variable representing time needs explicitly declared at the process of constructing a temporal data, referred to as "index". This accessibility of index promotes transparency and unambiguity while manipulating time. For example, subsetting a certain time period of data, extracting time components (like time of day and day of week), or converting time zones are directly dealt with index. It is also often to join other data tables based on the common time indices. Tools are provided to work with time itself, rather than wrappers to work with a whole data set. This promotes transparency and visibility to make inspection easier. This sets the tone on methods design philosophy: well-structured, expressive and unambiguous workflow.

For data indexed in regular time space, the time interval is obtained by computing the greatest common divisor based on the non-negative differences of a given time index, although the data may include implicit time gaps. This implies that all the units in the table must share a common interval. It is relatively easy to determine if the data is daily or sub-daily since most statistical packages have built-in time representations for date-times and dates, for example their corresponding classes `POSIXct` and `Date` in R. However these turn out inappropriate for weekly, monthly and quarterly data. For example, the smallest time unit of monthly data is months in a year, which is equispaced points in a year. However, using the first day of the month can range from 28 to 31 days, which is irregularly spaced. A genuine year-month representation should avoid associating days with its format. Displaying the year and month components suffice to signal its monthly aggregated values instead of observed values at a particular timestamp. This argument is also applicable to the creation of year-week and year-quarter. (coercion)

Since Table \@ref(tab:tb-sub) possesses complete observations for each category, the interval obtained from the greatest common factor is 1 year.

Extensible. Custom index classes can be supported.

## Key {#sec:key}

The "key" identifies units or subjects that are observed over time in a data table, which is typically a priori known structure by users. The absence of key only occurs to a univariate case where the key is implicit. Multiple units must associate with an explicit "key" to allow for identification. The key is not constrained to a single column, but is comprised of multiple columns. When involving multiple variables, one need to distinguish nesting and crossing data variables. "Nesting" refers to a scenario where a variable is nested within another when each category of the former variable co-occurs with only one category of the latter (for example, `country` nested within `continent`); on the other hand, "crossing" means that a variable is crossed with another when every category of one variable co-occurs with every category of the other (for example, `country` crosses with `gender`). Nesting is a special case of crossing. The former exhibits a hierarchical ordering with lower-level category (children) nested within higher-level category (parent) (more categories in lower level); whereas the ordering of the latter makes no difference. Whilst constructing tidy temporal data, users also need to specify the key that identifies the structure on the data. Rather than creating a separate object that contains the data structure, like what the **hts** does, we propose a syntax-based interface to directly deal with variables, so that it takes advantage of data semantics. The expression of a vertical bar (`|`) (can be interpreted probabilistically as the conditional distribution for its parent) is used to indicate nesting variables (reflect depths), while a comma (`,`) for crossing variables. @wilkinson2006grammar discussed the distinction between nesting and crossing with respect to facets in graphics, and he used `/` and `*` instead for nested and crossed expressions. A crossing structure displays all unique and possible combinations of crossed variables, including those not found in the data, in a graphical layout. However, tidy temporal data finds only the combinations that occur in the data and intentionally respects the structure of incomplete combinations.

# Data pipeline {#sec:pipeline}

There has been a long history of pipeline discussions and implementation centering around data in various aspects. The Extract, Transform, and Load (ETL) process is most commonly deployed pipeline in data science. However, building a sound data pipeline can be technically difficult, which involves many implementation decisions, including interface, input/output, functionality and so on, to be made.

@unix coined the term "pipelines" in software development, while developing Unix at Bell Labs. In Unix-based computer operating systems, a pipeline chains together a series of operations on the basis of their standard streams, so that the output of each programme becomes the input to another. This shapes the Unix toolbox philosophy: "each do one simple thing, do it well, and most importantly, work well with each other" [@unix-philosophy].

@viewing-pipeline proposed a viewing pipeline for data analysis in interactive statistical graphics and generalised a collection of elements required for the pipeline, which takes control of transformation from data to plot. @plumbing and @xie2014reactive implemented the data pipeline in interactive statistical software **plumbr** and **cranvas** respectively, using a reactive programming framework, in which user's interactions trigger a sequence of modules to update views accordingly.

What is notable about data pipeline is that it not only frees users from a tedious and error-prone analysis but also empowers a wider audience to focus on the statistical analysis tasks at hand without concerning the details of computational implementation. A fluent and coherent pipeline glues the grammar of data manipulation and tidy data as a fundamental unit of data analysis together. It helps (1) break up a big problem to into manageable blocks, (2) generate human readable analysis workflow, (3) avoid introducing mistakes as many as possible.

## Time-based pipeline

This "tidy" data representation choice will prove suitable for almost all conceivable applications of temporal data. But the data abstraction cannot live alone with an additional toolchain in order to be able to acquire insight and knowledge about the data itself. A desirable toolbox chains transformation, visualisation, and modelling as a data science pipeline. Each component itself is also a self-contained pipeline that can be decomposed into many individual operations. This paper will focus on part of the general pipeline---data transformation, for example filtering observations, selecting, and summarising variables, by contrast to data cleaning. We shall see how this data abstraction breeds a consistent data process.

All the individual operations strive to keep the time context so that they will implicitly preserve the index variable.

## Modularity and clarity

An individual operation that performs on a specific task can be phrased as a verb. This verb is self-explanatory enough to express what it is supposed to do or fail. We shall adapt **dplyr** key verbs to time domain and expand its vocabulary a little more to handle time-related analysis problems.

* **row-wise**: `filter()`, `slice()`, `arrange()`, `fill_na()`
* **column-wise**: `mutate()`, `select()`, `summarise()`
* **group-wise**: `index_by()`, `group_by()`
* **time-wise**: `lead()`, `lag()`, `difference()`
* **rolling window**: `slide()`, `tile()`, `stretch()`

## Composition and transparency

<!-- * [@Unwin99guiand] since programming interfaces are precise and repeatable, they are preferable when we can describe exactly what we want, but a GUI is better when: “Searching for information and interesting structures without fully specified questions.” [R] -->
* "No matter how complex and polished the individual operations are, it is often the quality of the glue that most directly determines the power of the system" [@eopl]

* The resulting code is well-structured, easier to read and comprehend. A way of chaining together simple operations to perform complex tasks in an efficient way. 

Individual operations can talk to each other.

A pipeline exhibits a hierarchy of data operations: (1) atomic (1-dimensional) vectors (`mean(variable)`) --> (2) (2-dimensional) data table (`summarise()`) --> repeat step (1) and (2) to form a chain. Reversely, a data pipeline is decomposed into rectangular blocks, and then into atomic strips.

The index and key will automate the update.

# Case studies

## U.S.A domestic flights on-time performance (2016-2017)

A dataset of on-time performance of domestic flights in U.S.A from 2016 to 2017 is studied and explored for illustration of tidy data and data pipeline.

# Conclusion and future work

A tidy representation of time series data, and data pipelines to facilitate data analysis flow have been proposed and discussed. It can be noted that tidy temporal data gains greater flexibility in keeping data richness, making data transformation and visualisation easily. A set of verbs provides a fluent and fluid pipeline to work with tidy time series data in various ways.

A new visualization framework for handling time series with nesting and crossing structure is under development. The proposed data structure proves a natural input (unit time series and parent-children structure) that can be piped into a visualization program. As long as the program understands the embedded structure, it will produce sensible layout and interactivity.

The ground of time series modelling or forecasting is left untouched in this paper. The future plan is to bridge the gap between tidy data and model building. Currently, it is required to casting to matrix from tidy data and therefore building a model. But time series models should be directly applied to tidy data as other wrangling tools do, without such an intermediate step. In particular, a univariate time series model, like ARIMA and Exponential Smoothing, can be applied to multiple time series independently. A tidy format to represent model summaries and forecasting objects will be developed and implemented in the future. Model summaries include coefficients, fitted values, and residuals; forecasting objects include future time path and distributions generating prediction intervals.

```{r write-bib}
write_bib(c("hts", "plm", "dplyr", "xts", "zoo"), file = "rpkgs.bib")
```

